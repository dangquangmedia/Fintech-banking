# app.py ‚Äî Credit Scoring (Streamlit) ‚Äî VN form 2 c·ªôt + CIC 300‚Äì850
# - Giao di·ªán ‚ÄúNh·∫≠p th√¥ng tin kh√°ch h√†ng‚Äù (2 c·ªôt), v·∫´n gi·ªØ c√°c tr∆∞·ªùng: Existing Loans, Delay days, Num Credit Cards,...
# - K·∫øt qu·∫£ hi·ªÉn th·ªã: PD v√† ƒëi·ªÉm t√≠n d·ª•ng theo CIC (300‚Äì850) + ph√¢n lo·∫°i (K√©m/Kh√°/T·ªët/R·∫•t t·ªët/Xu·∫•t s·∫Øc)
# - Gi·ªØ nguy√™n lu·ªìng train nhanh CSV, upload model, SHAP (gi·ªõi h·∫°n m·∫´u), v√† m·ªçi fix tr∆∞·ªõc (no-lambda pickling, √©p dtype/sparse cho SHAP‚Ä¶)
# - C√≥ th·ªÉ d√πng Logistic Regression ho·∫∑c Random Forest (t·ªëi ∆∞u hi·ªáu nƒÉng)

import os
from pathlib import Path
import numpy as np
import pandas as pd
import joblib
import streamlit as st

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, FunctionTransformer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix
from sklearn.exceptions import NotFittedError
from sklearn.utils.validation import check_is_fitted

import shap
import matplotlib.pyplot as plt

# ============== C·∫§U H√åNH ==============
# C√°c tr∆∞·ªùng demo kh·ªõp form + ∆∞u ti√™n nh·ªØng tr∆∞·ªùng b·∫°n y√™u c·∫ßu
FEATURE_SUBSET = [
    # Numeric
    "Age",
    "Annual_Income",
    "Monthly_Inhand_Salary",
    "Num_Bank_Accounts",
    "Num_Credit_Card",          # S·ªë th·∫ª t√≠n d·ª•ng
    "Interest_Rate",
    "Num_of_Loan",              # S·ªë kho·∫£n vay hi·ªán c√≥
    "Delay_from_due_date",      # S·ªë ng√†y tr·ªÖ h·∫°n
    # Categorical
    "Occupation",
    "Type_of_Loan",
]

MODEL_PATHS = [
    "models/random_forest.pkl",
    "models/logistic_regression.pkl",
    "models/uploaded.pkl",
    "models/xgboost.pkl",
]

# ============== H√ÄM H·ªñ TR·ª¢ ==============
def to_str(X): return X.astype(str)

def is_fitted_estimator(est)->bool:
    try:
        check_is_fitted(est); return True
    except Exception:
        pass
    for a in ("classes_","n_features_in_"):
        if hasattr(est,a): return True
    return False

def load_any_model():
    for p in MODEL_PATHS:
        if os.path.exists(p):
            try:
                mdl = joblib.load(p)
                if isinstance(mdl, Pipeline) and "clf" in mdl.named_steps:
                    if not is_fitted_estimator(mdl.named_steps["clf"]): continue
                else:
                    if not is_fitted_estimator(mdl): continue
                return mdl, p
            except Exception as e:
                st.warning(f"L·ªói n·∫°p model {p}: {e}")
    return None, None

def save_model(model,out_path:str):
    Path("models").mkdir(parents=True,exist_ok=True)
    joblib.dump(model,out_path); return out_path

def split_features_target(df,target_col):
    return df.drop(columns=[target_col]), df[target_col]

def infer_cols(X):
    num=[c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    cat=[c for c in X.columns if c not in num]
    return num,cat

def build_pipeline(model_type, X_sample, rf_params:dict|None=None)->Pipeline:
    rf_params = rf_params or {}
    num_cols, cat_cols = infer_cols(X_sample)
    num_tf = Pipeline([("imputer",SimpleImputer(strategy="median"))])
    cat_tf = Pipeline([
        ("imputer",SimpleImputer(strategy="most_frequent")),
        ("to_str",FunctionTransformer(lambda x: x.astype(str))),  # an to√†n pickling v√¨ FunctionTransformer —Å–µ—Ä–∏–∞–ª ho√° ƒë∆∞·ª£c
        ("ohe",OneHotEncoder(handle_unknown="ignore")),
    ])
    pre = ColumnTransformer([("num",num_tf,num_cols),("cat",cat_tf,cat_cols)])
    if model_type=="Logistic Regression":
        clf = LogisticRegression(max_iter=200)
    else:
        clf = RandomForestClassifier(
            n_estimators=rf_params.get("n_estimators",150),
            max_depth=rf_params.get("max_depth",None),
            max_features=rf_params.get("max_features","sqrt"),
            min_samples_leaf=rf_params.get("min_samples_leaf",1),
            random_state=42, n_jobs=-1
        )
    return Pipeline([("pre",pre),("clf",clf)])

def get_feature_names_from_pre(pre:ColumnTransformer)->list[str]:
    names=[]
    for name,trans,cols in pre.transformers_:
        if name=='remainder' and trans=='drop': continue
        if hasattr(trans,'named_steps'):
            last=list(trans.named_steps.values())[-1]
            if hasattr(last,'get_feature_names_out'):
                base=cols if isinstance(cols,list) else [cols]
                try: names+=last.get_feature_names_out(base).tolist()
                except Exception: names+= base
            else: names+= cols if isinstance(cols,list) else [cols]
        else: names+= cols if isinstance(cols,list) else [cols]
    return names

def ensure_subset_and_types(df):
    X=df.copy()
    for c in FEATURE_SUBSET:
        if c not in X.columns: X[c]=pd.NA
    X=X[FEATURE_SUBSET]
    for c in FEATURE_SUBSET:
        conv=pd.to_numeric(X[c],errors="coerce")
        X[c]=conv if conv.notna().any() else X[c].astype(str)
    return X

def densify_float64(X):
    if hasattr(X,"toarray"): X=X.toarray()
    return np.asarray(X,dtype=np.float64)

def try_predict(model,X_df):
    try:
        X_df=ensure_subset_and_types(X_df)
        if isinstance(model,Pipeline):
            pre,clf = model.named_steps["pre"], model.named_steps["clf"]
            X_tr=pre.transform(X_df)
            pred=clf.predict(X_tr)[0]
            proba=classes=None
            if hasattr(clf,"predict_proba"):
                pp=clf.predict_proba(X_tr)[0]; classes=getattr(clf,"classes_",[str(i) for i in range(len(pp))]); proba=pp
            return pred,proba,classes,X_tr,get_feature_names_from_pre(pre)
        else:
            pred=model.predict(X_df)[0]
            proba=classes=None
            if hasattr(model,"predict_proba"):
                pp=model.predict_proba(X_df)[0]; classes=getattr(model,"classes_",[str(i) for i in range(len(pp))]); proba=pp
            return pred,proba,classes,X_df.to_numpy(),list(X_df.columns)
    except NotFittedError:
        return "L·ªói: M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c hu·∫•n luy·ªán.",None,None,None,None
    except Exception as e:
        return f"L·ªói d·ª± ƒëo√°n: {e}",None,None,None,None

def plot_confusion_matrix_cm(y_true,y_pred,labels=None):
    cm=confusion_matrix(y_true,y_pred,labels=labels)
    fig,ax=plt.subplots(); im=ax.imshow(cm,interpolation='nearest'); ax.figure.colorbar(im,ax=ax)
    ax.set_title("Confusion Matrix"); ax.set_xlabel("Predicted"); ax.set_ylabel("True")
    ticks=np.arange(len(labels)) if labels is not None else np.arange(cm.shape[0])
    ax.set_xticks(ticks); ax.set_yticks(ticks)
    ax.set_xticklabels(labels if labels is not None else ticks,rotation=45,ha="right")
    ax.set_yticklabels(labels if labels is not None else ticks)
    thresh=cm.max()/2.0 if cm.size else .5
    for i in range(cm.shape[0]):
        for j in range(cm.shape[1]):
            ax.text(j,i,format(cm[i,j],"d"),ha="center",va="center",color="white" if cm[i,j]>thresh else "black")
    fig.tight_layout(); st.pyplot(fig)

def try_build_tree_explainer(pipe:Pipeline,X_bg):
    try:
        pre,clf = pipe.named_steps["pre"], pipe.named_steps["clf"]
        X_bg_tr = densify_float64(pre.transform(ensure_subset_and_types(X_bg)))
        if isinstance(clf,RandomForestClassifier):
            return shap.TreeExplainer(clf,feature_names=get_feature_names_from_pre(pre)), get_feature_names_from_pre(pre), X_bg_tr
    except Exception as e:
        st.info(f"Kh√¥ng t·∫°o ƒë∆∞·ª£c TreeExplainer: {e}")
    return None,None,None

def global_shap_bar(explainer,X_tr,feature_names,topk=15):
    X_tr=densify_float64(X_tr)
    shap_vals=explainer.shap_values(X_tr)
    if isinstance(shap_vals,list):
        abs_means=np.stack([np.mean(np.abs(sv),axis=0) for sv in shap_vals],axis=0); imp=abs_means.max(axis=0)
    else:
        imp=np.mean(np.abs(shap_vals),axis=0)
    idx=np.argsort(imp)[::-1][:topk]
    fig,ax=plt.subplots(); ax.barh([feature_names[i] for i in idx][::-1],imp[idx][::-1])
    ax.set_xlabel("|SHAP value| (mean)"); ax.set_title("Global feature importance (SHAP)")
    fig.tight_layout(); st.pyplot(fig)

def local_shap_table(explainer,x1_tr,feature_names,topk=10):
    x1=densify_float64(x1_tr).reshape(1,-1)
    shap_vals=explainer.shap_values(x1)
    if isinstance(shap_vals,list):
        totals=[np.sum(np.abs(sv)) for sv in shap_vals]; sv=shap_vals[int(np.argmax(totals))][0]
    else: sv=shap_vals[0]
    order=np.argsort(np.abs(sv))[::-1][:topk]
    st.subheader("ƒê√≥ng g√≥p theo ƒë·∫∑c tr∆∞ng (SHAP) ‚Äì b·∫£n ghi hi·ªán t·∫°i")
    st.dataframe(pd.DataFrame({"feature":[feature_names[i] for i in order],
                               "shap_value":sv[order],
                               "contribution":np.where(sv[order]>=0,"‚Üë tƒÉng r·ªßi ro","‚Üì gi·∫£m r·ªßi ro")}),
                 use_container_width=True)

def fake_lookup(identifier:str)->pd.DataFrame:
    return pd.DataFrame([{
        "Age":30,"Annual_Income":30000,"Monthly_Inhand_Salary":2000,"Num_Bank_Accounts":2,
        "Num_Credit_Card":1,"Interest_Rate":12,"Num_of_Loan":1,"Occupation":"Employee",
        "Type_of_Loan":"Personal","Delay_from_due_date":0
    }])

def ensure_frame(x): return pd.DataFrame([x]) if isinstance(x,dict) else x

# ----- ƒêi·ªÉm CIC 300‚Äì850 -----
def pd_to_cic_score(pd_hat: float) -> int:
    """Map PD (0..1) -> ƒëi·ªÉm CIC 300..850 (cao = r·ªßi ro th·∫•p)."""
    pd_hat = float(np.clip(pd_hat, 0.0, 1.0))
    return int(round(300 + (1.0 - pd_hat) * 550))  # span 550

def classify_cic(score: int) -> str:
    if score >= 800: return "Xu·∫•t s·∫Øc (800‚Äì850)"
    if score >= 740: return "R·∫•t t·ªët (740‚Äì799)"
    if score >= 670: return "T·ªët (670‚Äì739)"
    if score >= 580: return "Kh√° (580‚Äì669)"
    return "K√©m (<580)"

def estimate_pd_from_proba(classes, proba) -> float:
    """∆Øu ti√™n x√°c su·∫•t l·ªõp r·ªßi ro cao: Poor/Bad. N·∫øu c√≥ 'Standard' xem nh∆∞ r·ªßi ro trung b√¨nh 0.5."""
    if classes is None or proba is None: return 0.5
    label_to_w = {"poor":1.0, "bad":1.0, "standard":0.5, "good":0.0}
    pd_est = 0.0
    for c,p in zip(classes, proba):
        w = label_to_w.get(str(c).lower(), 0.0)
        pd_est += w * float(p)
    # n·∫øu c√°c l·ªõp ch·ªâ c√≥ 2 (good/poor) th√¨ gi√° tr·ªã tr√™n l√† h·ª£p l√Ω;
    # n·∫øu ƒë·ªß 3 l·ªõp (good/standard/poor) th√¨ coi standard ~0.5 * p
    return float(np.clip(pd_est, 0.0, 1.0))

# ============== UI ==============
st.set_page_config(page_title="ƒê√°nh gi√° ƒëi·ªÉm t√≠n d·ª•ng", layout="wide")

st.sidebar.header("Ch·∫ø ƒë·ªô")
mode=st.sidebar.radio("Ch·ªçn ch·∫ø ƒë·ªô",["Tra nhanh (SƒêT/CCCD/ID)","Ch·∫•m ƒëi·ªÉm chi ti·∫øt","Gi·∫£i th√≠ch & ƒê·∫°o ƒë·ª©c","Qu·∫£n tr·ªã (train & upload)"])
model,model_path=load_any_model()
if "explainer" not in st.session_state: st.session_state.explainer=None
if "feature_names" not in st.session_state: st.session_state.feature_names=None
st.sidebar.success(f"Model: {os.path.basename(model_path)}" if model_path else "Ch∆∞a c√≥ model ‚Äì m√¥ ph·ªèng")

# ============== 1) TRA NHANH ==============
if mode=="Tra nhanh (SƒêT/CCCD/ID)":
    st.title("Tra nhanh ƒëi·ªÉm t√≠n d·ª•ng")
    st.write("- Dataset Kaggle th∆∞·ªùng **kh√¥ng c√≥ SƒêT** ‚Üí h√£y nh·∫≠p **ID/Customer_ID**. N·∫øu kh√¥ng upload CSV, h·ªá th·ªëng d√πng demo gi·∫£ l·∫≠p.")
    identifier=st.text_input("Nh·∫≠p SƒêT/CCCD/ID:")
    st.markdown("**(Tu·ª≥ ch·ªçn)** T·∫£i dataset CSV ƒë·ªÉ tra c·ª©u b·∫£n ghi th·∫≠t:")
    search_csv=st.file_uploader("Upload CSV (c√≥ c·ªôt ID/SƒêT/CCCD/Customer_ID)",type=["csv"],key="search_csv")
    id_column=df_search=None
    if search_csv is not None:
        df_search=pd.read_csv(search_csv)
        st.dataframe(df_search.head())
        guess=[c for c in df_search.columns if c.lower() in ("id","customer_id","phone","cccd","sdt")]
        id_column=st.selectbox("Ch·ªçn c·ªôt ƒë·ªãnh danh ƒë·ªÉ tra",options=df_search.columns,
                               index=0 if not guess else df_search.columns.get_loc(guess[0]))
    if st.button("Ki·ªÉm tra"):
        if not identifier.strip():
            st.warning("Vui l√≤ng nh·∫≠p ID/Customer_ID.")
        else:
            if df_search is not None and id_column:
                row=df_search[df_search[id_column].astype(str)==str(identifier)].drop(columns=[id_column],errors="ignore")
                if row.empty:
                    st.error("Kh√¥ng t√¨m th·∫•y h·ªì s∆° trong CSV."); st.stop()
                X=row
            else:
                X=ensure_frame(fake_lookup(identifier))
            if model is None:
                X=ensure_subset_and_types(X)
                s=(X.get("Annual_Income",pd.Series([30000])).iloc[0]/10000 +
                   X.get("Monthly_Inhand_Salary",pd.Series([2000])).iloc[0]/1000 -
                   X.get("Num_of_Loan",pd.Series([1])).iloc[0]*0.5 -
                   X.get("Delay_from_due_date",pd.Series([0])).iloc[0]/20 -
                   X.get("Interest_Rate",pd.Series([12])).iloc[0]/30)
                st.info(f"K·∫øt qu·∫£ (m√¥ ph·ªèng): **{'T·ªët' if s>=1.5 else ('Trung b√¨nh' if s>=0.9 else 'K√©m')}**")
            else:
                pred,proba,classes,X_tr,feat_names=try_predict(model,X)
                if isinstance(pred,str) and pred.startswith("L·ªói"): st.error(pred)
                else:
                    pd_hat = estimate_pd_from_proba(classes, proba)
                    score = pd_to_cic_score(pd_hat)
                    st.success("K·∫øt qu·∫£")
                    c1,c2,c3 = st.columns(3)
                    c1.metric("D·ª± ƒëo√°n l·ªõp", str(pred))
                    c2.metric("PD (x√°c su·∫•t v·ª° n·ª£)", f"{pd_hat:.3f}")
                    c3.metric("ƒêi·ªÉm CIC (300‚Äì850)", f"{score} ¬∑ {classify_cic(score)}")
                    if st.session_state.explainer is not None and X_tr is not None:
                        with st.expander("Gi·∫£i th√≠ch (SHAP) ‚Äì b·∫£n ghi hi·ªán t·∫°i"):
                            local_shap_table(st.session_state.explainer,X_tr,st.session_state.feature_names,topk=10)
                    else: st.caption("G·ª£i √Ω: Train Random Forest ·ªü tab Qu·∫£n tr·ªã ƒë·ªÉ b·∫≠t SHAP.")

# ============== 2) CH·∫§M ƒêI·ªÇM CHI TI·∫æT ‚Äî GIAO DI·ªÜN 2 C·ªòT ==============
elif mode=="Ch·∫•m ƒëi·ªÉm chi ti·∫øt":
    st.title("Nh·∫≠p th√¥ng tin kh√°ch h√†ng")  # gi·ªØ ti√™u ƒë·ªÅ theo y√™u c·∫ßu
    st.caption("ƒêi·ªÅn th√¥ng tin theo ti√™u ch√≠. K·∫øt qu·∫£ tr·∫£ PD v√† ƒëi·ªÉm CIC 300‚Äì850.")

    with st.form("detail_form"):
        left, right = st.columns(2, gap="large")

        # Nh√≥m numeric (b√™n tr√°i)
        age         = left.number_input("Tu·ªïi", 18, 100, 30, help="ƒê·ªô tu·ªïi kh√°ch h√†ng.")
        income      = left.number_input("Annual Income", 1000, 10_000_000, 30000, help="Thu nh·∫≠p nƒÉm.")
        inhand      = left.number_input("Monthly Inhand Salary", 100, 100_000, 2000, help="Thu nh·∫≠p th·ª±c lƒ©nh/th√°ng.")
        num_acc     = left.number_input("S·ªë t√†i kho·∫£n ng√¢n h√†ng", 0, 50, 2)
        num_card    = left.number_input("S·ªë th·∫ª t√≠n d·ª•ng", 0, 50, 1)  # GI·ªÆ
        rate        = left.number_input("L√£i su·∫•t (%)", 0, 100, 12)
        num_loan    = left.number_input("S·ªë kho·∫£n vay hi·ªán c√≥", 0, 50, 1)  # GI·ªÆ
        delay       = left.number_input("S·ªë ng√†y tr·ªÖ h·∫°n", 0, 3650, 0)      # GI·ªÆ

        # Nh√≥m categorical (b√™n ph·∫£i)
        occupation  = right.text_input("Ngh·ªÅ nghi·ªáp", "Employee")
        loan_type   = right.text_input("Lo·∫°i kho·∫£n vay", "Personal")

        submitted = st.form_submit_button("D·ª± ƒëo√°n")

    X = ensure_frame({
        "Age": age,
        "Annual_Income": income,
        "Monthly_Inhand_Salary": inhand,
        "Num_Bank_Accounts": num_acc,
        "Num_Credit_Card": num_card,
        "Interest_Rate": rate,
        "Num_of_Loan": num_loan,
        "Delay_from_due_date": delay,
        "Occupation": occupation,
        "Type_of_Loan": loan_type,
    })

    if submitted:
        if model is None:
            X2=ensure_subset_and_types(X)
            # m√¥ ph·ªèng ƒë∆°n gi·∫£n (c√≥ tr·ªçng s·ªë delay + num_card + num_loan)
            s = (income/10000 + inhand/1000 - num_loan*0.6 - delay/18 - rate/30 - num_card*0.1)
            pd_hat = float(1.0/(1.0+np.exp(s)))  # logistic gi·∫£ l·∫≠p
            score = pd_to_cic_score(pd_hat)
            c1,c2,c3 = st.columns(3)
            c1.metric("D·ª± ƒëo√°n (m√¥ ph·ªèng)", "Kh√¥ng c√≥ model")
            c2.metric("PD (x√°c su·∫•t v·ª° n·ª£)", f"{pd_hat:.3f}")
            c3.metric("ƒêi·ªÉm CIC (300‚Äì850)", f"{score} ¬∑ {classify_cic(score)}")
        else:
            pred,proba,classes,X_tr,feat_names=try_predict(model,X)
            if isinstance(pred,str) and pred.startswith("L·ªói"): st.error(pred)
            else:
                pd_hat = estimate_pd_from_proba(classes, proba)
                score = pd_to_cic_score(pd_hat)
                st.success("K·∫øt qu·∫£")
                c1,c2,c3 = st.columns(3)
                c1.metric("D·ª± ƒëo√°n l·ªõp", str(pred))
                c2.metric("PD (x√°c su·∫•t v·ª° n·ª£)", f"{pd_hat:.3f}")
                c3.metric("ƒêi·ªÉm CIC (300‚Äì850)", f"{score} ¬∑ {classify_cic(score)}")
                if st.session_state.explainer is not None and X_tr is not None:
                    with st.expander("Gi·∫£i th√≠ch (SHAP) ‚Äì b·∫£n ghi hi·ªán t·∫°i"):
                        local_shap_table(st.session_state.explainer,X_tr,st.session_state.feature_names,topk=10)
                else:
                    st.caption("G·ª£i √Ω: Train Random Forest ·ªü tab Qu·∫£n tr·ªã ƒë·ªÉ b·∫≠t SHAP.")

# ============== 3) GI·∫¢I TH√çCH & ƒê·∫†O ƒê·ª®C ==============
elif mode=="Gi·∫£i th√≠ch & ƒê·∫°o ƒë·ª©c":
    st.title("Gi·∫£i th√≠ch & ƒê·∫°o ƒë·ª©c")
    st.markdown("""
**Minh b·∫°ch**: gi·∫£i th√≠ch c·ª•c b·ªô (SHAP) cho t·ª´ng d·ª± ƒëo√°n; c∆° ch·∫ø so√°t x√©t/kh√°ng ngh·ªã.  
**Quy·ªÅn ri√™ng t∆∞**: t√°ch PII, ·∫©n danh, h·∫°n ch·∫ø truy c·∫≠p.  
**C√¥ng b·∫±ng**: theo d√µi bias theo nh√≥m; c√¢n b·∫±ng d·ªØ li·ªáu; ƒëi·ªÅu ch·ªânh ng∆∞·ª°ng.  
**B·∫£o m·∫≠t**: t√°ch d·ªãch v·ª• scoring; ki·ªÉm so√°t truy c·∫≠p; log ·∫©n danh.
    """)

# ============== 4) QU·∫¢N TR·ªä (TRAIN & UPLOAD) ==============
else:
    st.title("Qu·∫£n tr·ªã (train & upload)")
    st.subheader("A) Upload model c√≥ s·∫µn (.pkl)")
    f=st.file_uploader("Upload model (.pkl)",type=["pkl"])
    if f:
        out=Path("models/uploaded.pkl"); out.parent.mkdir(parents=True,exist_ok=True); out.write_bytes(f.read())
        st.success(f"ƒê√£ l∆∞u model: {out}")
        try:
            new_model,new_path=load_any_model()
            if new_model: st.info(f"Model ƒëang d√πng: {new_path}")
        except Exception as e: st.error(f"L·ªói n·∫°p model upload: {e}")

    st.markdown("---")
    st.subheader("B) Train nhanh t·ª´ CSV (Kaggle/ngu·ªìn kh√°c)")
    up=st.file_uploader("Upload CSV Train",type=["csv"],key="train_csv")
    if up is not None:
        df_train=pd.read_csv(up)
        st.write("Xem tr∆∞·ªõc d·ªØ li·ªáu:"); st.dataframe(df_train.head())

        # ch·ªçn target & ID
        guess_t=next((c for c in ["Credit_Score","risk_flag","label","target"] if c in df_train.columns),None)
        target_col=st.selectbox("Ch·ªçn c·ªôt nh√£n (target)",options=df_train.columns,
                                index=df_train.columns.get_loc(guess_t) if guess_t else 0)
        guess_id=next((c for c in ["ID","Customer_ID","customer_id"] if c in df_train.columns),None)
        id_col=st.selectbox("Ch·ªçn c·ªôt ID (tu·ª≥ ch·ªçn, s·∫Ω b·ªè khi train)",
                            options=["(Kh√¥ng d√πng)"]+list(df_train.columns),
                            index=(["(Kh√¥ng d√πng)"]+list(df_train.columns)).index(guess_id) if guess_id else 0)

        df_work=df_train.drop(columns=[id_col]) if id_col!="(Kh√¥ng d√πng)" else df_train.copy()

        missing=[c for c in FEATURE_SUBSET if c not in df_work.columns]
        if missing:
            st.error("Dataset thi·∫øu c√°c c·ªôt cho demo (kh·ªõp form): "+", ".join(missing)); st.stop()

        df_model=df_work[FEATURE_SUBSET+[target_col]].copy()
        for c in FEATURE_SUBSET:
            conv=pd.to_numeric(df_model[c],errors="coerce")
            df_model[c]=conv if conv.notna().any() else df_model[c].astype(str)

        # Tu·ª≥ ch·ªçn gi·∫£m c·ª° d·ªØ li·ªáu (ƒë·ªÉ nhanh)
        reduce_frac = st.selectbox("Gi·∫£m k√≠ch th∆∞·ªõc t·∫≠p train (ƒë·ªÉ nhanh)", ["1.0 (kh√¥ng gi·∫£m)","0.5","0.25","0.1"], index=0)
        reduce_frac = float(reduce_frac.split()[0])
        X_all,y_all=split_features_target(df_model,target_col)
        if reduce_frac<1.0:
            tmp=X_all.copy(); tmp[target_col]=y_all.values
            X_all = tmp.groupby(target_col, group_keys=False).apply(lambda d: d.sample(max(1,int(len(d)*reduce_frac)), random_state=42)).drop(columns=[target_col])
            y_all = tmp.groupby(target_col, group_keys=False).apply(lambda d: d.sample(max(1,int(len(d)*reduce_frac)), random_state=42))[target_col]

        test_size=st.slider("T·ªâ l·ªá test",0.1,0.4,0.2,step=0.05)
        model_type=st.selectbox("Ch·ªçn thu·∫≠t to√°n",["Logistic Regression","Random Forest"])

        rf_params={}
        enable_shap=False
        if model_type=="Random Forest":
            rf_params["n_estimators"]=st.slider("S·ªë c√¢y Random Forest",50,400,150,step=50)
            md = st.selectbox("max_depth",["None","5","10","20"],index=0)
            rf_params["max_depth"]=None if md=="None" else int(md)
            mf = st.selectbox("max_features",["sqrt","log2","0.5"],index=0)
            rf_params["max_features"]=0.5 if mf=="0.5" else mf
            rf_params["min_samples_leaf"]=st.selectbox("min_samples_leaf",[1,2,4,8],index=1)
            enable_shap = st.checkbox("B·∫≠t SHAP (gi·ªõi h·∫°n m·∫´u, c√≥ th·ªÉ ch·∫≠m)", value=False)

        if st.button("Hu·∫•n luy·ªán"):
            try:
                vc=y_all.value_counts(); strat=None if (vc<2).any() else y_all
                X_tr,X_te,y_tr,y_te=train_test_split(X_all,y_all,test_size=test_size,random_state=42,stratify=strat)
                pipe=build_pipeline(model_type,X_tr,rf_params=rf_params)

                with st.status("ƒêang hu·∫•n luy·ªán...",expanded=True) as status:
                    pipe.fit(X_tr,y_tr); st.write("‚úÖ ƒê√£ fit xong model.")
                    y_pred=pipe.predict(X_te)
                    acc=accuracy_score(y_te,y_pred); f1=f1_score(y_te,y_pred,average="weighted")
                    st.write("**K·∫øt qu·∫£ tr√™n t·∫≠p test:**"); st.write(f"- Accuracy: {acc:.4f}"); st.write(f"- F1 (weighted): {f1:.4f}")
                    st.code(classification_report(y_te,y_pred),language="text")
                    with st.expander("Confusion Matrix"):
                        labels=[str(c) for c in sorted(pd.unique(pd.concat([y_tr,y_te])))]
                        plot_confusion_matrix_cm(y_te,y_pred,labels=labels)

                    st.session_state.explainer=None; st.session_state.feature_names=None
                    if model_type=="Random Forest" and enable_shap:
                        st.write("üîé ƒêang t·∫°o SHAP (gi·ªõi h·∫°n m·∫´u)‚Ä¶")
                        bg_n=min(150,len(X_tr)); te_n=min(200,len(X_te))
                        X_bg=X_tr.sample(bg_n,random_state=42) if len(X_tr)>bg_n else X_tr
                        X_te_small=X_te.sample(te_n,random_state=42) if len(X_te)>te_n else X_te
                        explainer,feat_names,_=try_build_tree_explainer(pipe,X_bg)
                        if explainer is not None:
                            st.session_state.explainer=explainer; st.session_state.feature_names=feat_names
                            with st.expander(f"Global Feature Importance (SHAP) ‚Äî {te_n} m·∫´u"):
                                pre=pipe.named_steps["pre"]
                                X_te_tr=densify_float64(pre.transform(ensure_subset_and_types(X_te_small)))
                                global_shap_bar(explainer,X_te_tr,feat_names,topk=15)
                        else: st.info("Kh√¥ng t·∫°o ƒë∆∞·ª£c SHAP Explainer.")
                    status.update(label="Hu·∫•n luy·ªán xong!",state="complete")

                out_name="models/logistic_regression.pkl" if model_type=="Logistic Regression" else "models/random_forest.pkl"
                save_model(pipe,out_name); st.success(f"üíæ ƒê√£ l∆∞u model: {out_name}")

            except Exception as e:
                st.error(f"L·ªói train: {e}")
